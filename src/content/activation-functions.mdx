export const meta = {
  title: "Activation Functions",
  subtitle: "A chronological tour through non-linearity in neural networks",
  date: "2025-12-25",
  authors: ["Eli Plutchok", "Isaac Trenk"],
  slug: "activation-functions-in-neural-networks"
}

## Why activation functions?

An activation function is a **non-linear** function applied inside a neural network, typically right after a linear transformation. A single layer computes:

$$z = Wx + b, \quad a = \phi(z)$$

where $W$ and $b$ define a linear map and $\phi$ is the activation function. The activation function is usually applied element-wise:

$$\phi([z_1, z_2, \ldots, z_n]) = [\phi(z_1), \phi(z_2), \ldots, \phi(z_n)]$$

Why is non-linearity necessary? Without it, we can only represent linear functions. A linear function is something like $ax + by$: each input contributes independently, and changes in input produce a constant change in output. Increase $x$ by 1, and the output always changes by the same amount $a$, regardless of what $x$ or $y$ currently are.

This is limiting. Here are some things that a linear function cannot do:

- have the rate of change vary as the input changes
- have thresholds where something significant happens at the output
- capture interactions between inputs, like $x \cdot y$

When we add non-linear activation functions after linear layers, we can start doing more interesting things. A single linear layer followed by an activation function is still quite limited in what it can represent. But the key insight is that we can stack these layers. Each layer applies a linear transformation followed by a non-linearity, and by composing many such layers, we can approximate arbitrarily complex non-linear functions <Cite authors="Cybenko" year="1989" url="https://doi.org/10.1007/BF02551274" />.

You might wonder: why not just stack linear layers without activation functions? The problem is that composing linear functions produces another linear function. Consider two layers where the output of the first feeds into the second:

$$
\begin{aligned}
y &= W_2 (W_1 x + b_1) + b_2 \\
  &= W_2 W_1 x + W_2 b_1 + b_2 \\
  &= W' x + b'
\end{aligned}
$$

where $W' = W_2 W_1$ and $b' = W_2 b_1 + b_2$. The two layers collapse into a single linear transformation.

No matter how many layers you stack, the result is equivalent to a single linear transformation. Depth buys you nothing. The activation functions are what prevent this collapse. They ensure that each layer's contribution cannot be absorbed into the next, allowing the network to build up complex representations layer by layer.

<NeuralNetworkDemo />

The earliest activation functions were inspired by biology. Over time, that motivation faded, and what mattered was the math. Here's how they evolved.

## Step function (1943)

The first neural network models aimed to mimic biological neurons. A neuron receives inputs, weighs them, and "fires" if the total exceeds a threshold. McCulloch and Pitts formalized this as <Cite authors="McCulloch & Pitts" year="1943" url="https://doi.org/10.1007/BF02478259" />:

$$y = \text{step}\left(\sum_i w_i x_i - \theta\right)$$

where $w_i$ are weights, $x_i$ are inputs, and $\theta$ is a threshold. The step function outputs 1 if the weighted sum exceeds the threshold, and 0 otherwise:

$$\text{step}(z) = \begin{cases} 1 & z \ge 0 \\ 0 & z < 0 \end{cases}$$

<ActivationGraph type="step" />

Rosenblatt's perceptron <Cite authors="Rosenblatt" year="1958" url="https://doi.org/10.1037/h0042519" /> used this activation and could learn simple classifications through a weight update rule.

But the step function has a fatal flaw for gradient-based learning. Its derivative is zero everywhere except at the threshold, where it's undefined. This creates two problems: gradients provide no signal for how to adjust weights, and even if we somehow knew which direction to move, the output jumps discontinuously. There's no way to make small, controlled updates. Backpropagation cannot work under these conditions.

## Sigmoid (1986)

The solution was to smooth the step into a differentiable curve. The **logistic sigmoid** became the standard activation for early multi-layer networks trained with backpropagation <Cite authors="Rumelhart, Hinton & Williams" year="1986" url="https://doi.org/10.1038/323533a0" />:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

<ActivationGraph type="sigmoid" />

Sigmoid behaves like a soft step: large negative inputs map near 0, large positive inputs map near 1, with a smooth transition between. Its derivative is well-defined everywhere:

$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$

This allowed gradients to flow backward through networks, enabling training of deeper architectures. The output range $(0, 1)$ also made sigmoid natural for probabilistic outputs.

However, sigmoid saturates at extreme values, so the derivative approaches zero, causing gradients to vanish during backpropagation. This made training deep networks slow and difficult.

## Tanh

A simple improvement: shift sigmoid to be zero-centered.

$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

<ActivationGraph type="tanh" yRange={[-1.5, 1.5]} />

Tanh maps inputs to $(-1, 1)$ instead of $(0, 1)$. Zero-centered outputs often help optimization since gradients don't systematically push weights in one direction. But tanh still saturates at large magnitudes, so the vanishing gradient problem persists.

## ReLU (2010)

For decades, sigmoid and tanh dominated. Then came a surprisingly simple alternative that changed everything:

$$\text{ReLU}(z) = \max(0, z)$$

<ActivationGraph type="relu" yRange={[-1, 5]} />

ReLU (the Rectified Linear Unit) outputs zero for negative inputs and passes positive inputs unchanged <Cite authors="Nair & Hinton" year="2010" url="https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf" />. It never saturates for positive values, so gradients flow freely. It's also computationally trivial, just a comparison and a maximum.

ReLU made training deep networks dramatically easier. It became the default activation for most architectures.

The main drawback is "dying ReLU": if a neuron's inputs consistently land in the negative region, it outputs zero and receives zero gradients. Once dead, it stays dead.

## Leaky ReLU

A simple fix for dying neurons: allow a small gradient for negative inputs.

$$\text{LeakyReLU}(z) = \max(\alpha z, z), \quad \alpha \approx 0.01$$

<ActivationGraph type="leakyRelu" yRange={[-1, 5]} />

Instead of flatlining at zero, negative inputs produce a small negative output. This keeps neurons alive during training while preserving ReLU's benefits.

## GELU (2016)

Modern architectures like Transformers use smoother alternatives. GELU gates inputs probabilistically based on their magnitude <Cite authors="Hendrycks & Gimpel" year="2016" url="https://arxiv.org/abs/1606.08415" />:

$$\text{GELU}(z) = z \cdot \Phi(z)$$

where $\Phi(z)$ is the cumulative distribution function of the standard normal.

<ActivationGraph type="gelu" yRange={[-1, 5]} />

Unlike ReLU's hard cutoff at zero, GELU smoothly transitions. Small negative values are partially passed through rather than completely zeroed. This matches the behavior of stochastic regularization techniques like dropout and has become standard in large language models.

## Summary

Activation functions exist to introduce non-linearity. The story is one of gradual refinement: from biologically-inspired step functions that couldn't be trained with gradients, to smooth sigmoids that enabled backpropagation, to ReLU that finally made deep networks practical, to modern variants tuned for specific architectures.

The biological motivation faded long ago. What remains is a simple requirement: break linearity so that depth means something.
