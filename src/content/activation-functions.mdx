export const meta = {
  title: "Activation Functions",
  subtitle: "An introduction and brief history",
  date: "2025-12-25",
  authors: ["Eli Plutchok", "Isaac Trenk"],
  slug: "activation-functions-in-neural-networks"
}

***Activation functions are non-linear functions inserted between the layers of a neural network allowing it to learn complex patterns.***

That sentence is quite dense so here are the relevant definitions:

- **Function.** A rule that takes an input and produces an output. Functions are often represented with symbols like $f$, so $f(x)$ means "apply the function $f$ to the input $x$." In neural networks, the symbol $\phi$ (the Greek letter "phi") is commonly used to denote an activation function, so $\phi(z)$ means "apply the activation function to $z$." Some examples:
  - **$f(x) = x^2$**. Takes a number and returns its square. Input: $3$, output: $9$.
  - **$f(x) = \text{uppercase}(x)$**. Takes a string and returns it in uppercase. Input: "hello", output: "HELLO".
  - **$f(\mathbf{v}) = \|\mathbf{v}\|$**. Takes a vector and returns its length. Input: $[3, 4]$, output: $5$.

- **Linear function.** A function where each input contributes independently and changes in input produce a constant change in output. Some examples:
  - **$f(x) = 3x + 2$**. Multiply by 3 and add 2. Increasing $x$ by 1 always increases the output by exactly 3, no matter what $x$ is.
  - **$f(x_1, x_2) = 4x_1 - x_2 + 7$**. Each input is scaled and added. Changing $x_1$ by 1 always changes the output by 4, regardless of $x_2$.
  - **$f(x) = x$**. The identity function. The output equals the input; the rate of change is always exactly 1.

- **Non-linear function.** A function where the rate of change can vary depending on the input, and inputs can interact with each other. Some examples:
  - **$f(x) = x^2$**. The rate of change depends on $x$ itself. Near $x = 1$ a small increase barely changes the output, but near $x = 100$ the same increase produces a much larger change.
  - **$f(x_1, x_2) = x_1 \cdot x_2$**. The two inputs interact. How much $x_1$ affects the output depends on the current value of $x_2$, and vice versa.
  - **$f(x) = \max(0, x)$**. Outputs zero for any negative input and passes positive inputs unchanged. The behavior is fundamentally different on each side of zero.

- **Neural network.** A function that takes some input and produces an output. Rather than being defined all at once, it is composed of many similar sub-functions (called layers) chained together, where the output of one layer becomes the input to the next.

- **Single layer of a neural network.** Each layer is itself a function. It computes:

  $$z = Wx + b, \quad a = \phi(z)$$

  Here $x$ is the input to the layer, a vector (a list of numbers). $W$ is a **weight matrix** (a grid of numbers that linearly mixes the inputs), $b$ is a **bias vector** (a set of numbers added after the mixing to shift the result), and $\phi$ is the activation function. Together, $Wx + b$ is the linear part of the layer, and $\phi$ is the non-linear part.

- **Activation function.** The non-linear function $\phi$ applied after the linear transformation in each layer. It is typically applied element-wise, meaning it operates on each value independently:

  $$\phi([z_1, z_2, \ldots, z_n]) = [\phi(z_1), \phi(z_2), \ldots, \phi(z_n)]$$

  The left side feeds a whole list of values into $\phi$; the right side shows that $\phi$ is just applied to each element separately, producing a new list of the same size. For example, the **sigmoid** function is one common activation function:

  $$\sigma(z) = \frac{1}{1 + e^{-z}}$$

  To use it, substitute your input for $z$ on the right-hand side â€” for instance, $\sigma(3) = \frac{1}{1 + e^{-3}} \approx 0.95$. We'll see a graph of sigmoid and a deeper discussion of it later in this article.

Ok, end of definitions.

## Why is the non-linearity necessary?

Without activation functions, we can only represent linear functions. Linear functions are quite limited.

- They cannot have the rate of change vary as the input changes
- They cannot have thresholds where something significant happens at the output
- They cannot capture interactions between inputs, like $x_1 \cdot x_2$

When we add non-linear activation functions after each linear layer, we can start doing these more interesting things that we just mentioned. And while a single layer is still quite limited, we can stack these layers to create ever more complex functions. Each layer applies a linear transformation followed by a non-linearity, and by composing many such layers, or even just two with enough neurons, we can approximate arbitrarily complex non-linear functions <Cite authors="Cybenko" year="1989" url="https://doi.org/10.1007/BF02551274" />.

You might wonder why we can't just stack linear layers without activation functions. The problem is that composing linear functions always produces another linear function. Consider two layers where the output of the first feeds into the second.

$$
\begin{aligned}
y &= W_2 (W_1 x + b_1) + b_2 \\
  &= W_2 W_1 x + W_2 b_1 + b_2 \\
  &= W' x + b'
\end{aligned}
$$

where $W' = W_2 W_1$ and $b' = W_2 b_1 + b_2$. The two layers collapse into a single linear transformation.

No matter how many layers you stack, the result is equivalent to a single linear transformation. Depth buys you nothing. The activation functions are what prevent this collapse. They ensure that each layer's contribution cannot be absorbed into the next, allowing the network to build up complex representations layer by layer.

Here is an interactive demo of a small neural network where you can see how the activation function affects the output:

<NeuralNetworkDemo />

## A brief history of activation functions

The earliest activation functions were inspired by biology. Over time, that motivation faded, and what mattered was the math. Here's how they evolved.

## Step function (1943)

The first neural network models aimed to mimic biological neurons. A neuron receives inputs, weighs them, and "fires" if the total exceeds a threshold. McCulloch and Pitts formalized this as <Cite authors="McCulloch & Pitts" year="1943" url="https://doi.org/10.1007/BF02478259" />

$$y = \text{step}\left(\sum_i w_i x_i - \theta\right)$$

where $w_i$ are weights, $x_i$ are inputs, and $\theta$ is a threshold. The step function outputs 1 if the weighted sum exceeds the threshold, and 0 otherwise.

$$\text{step}(z) = \begin{cases} 1 & z \ge 0 \\ 0 & z < 0 \end{cases}$$

<ActivationGraph type="step" />

Rosenblatt's perceptron <Cite authors="Rosenblatt" year="1958" url="https://doi.org/10.1037/h0042519" /> used this activation and could learn simple classifications through a weight update rule.

But the step function has a fatal flaw for gradient-based learning. Its derivative is zero everywhere except at the threshold, where it's undefined. This creates two problems. Gradients provide no signal for how to adjust weights, and even if we somehow knew which direction to move, the output jumps discontinuously. There's no way to make small, controlled updates. Backpropagation cannot work under these conditions.

## Sigmoid (1986)

The solution was to smooth the step into a differentiable curve. The **logistic sigmoid** became the standard activation for early multi-layer networks trained with backpropagation <Cite authors="Rumelhart, Hinton & Williams" year="1986" url="https://doi.org/10.1038/323533a0" />.

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

<ActivationGraph type="sigmoid" />

Sigmoid behaves like a soft step: large negative inputs map near 0, large positive inputs map near 1, with a smooth transition between. Its derivative is well-defined everywhere.

$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$

This allowed gradients to flow backward through networks, enabling training of deeper architectures. The output range $(0, 1)$ also made sigmoid natural for probabilistic outputs.

However, sigmoid has two issues. First, it saturates at extreme values, so the derivative approaches zero, causing gradients to vanish during backpropagation. Second, its outputs are always positive, which can cause optimization problems when gradients tend to push all weights in the same direction <Cite authors="LeCun et al." year="1998" url="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" />.

## Tanh

Tanh is essentially a rescaled sigmoid that maps inputs to $(-1, 1)$ instead of $(0, 1)$.

$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

<ActivationGraph type="tanh" yRange={[-1.5, 1.5]} />

With outputs centered around zero, the optimization issues from always-positive activations go away. Tanh became the default activation for many years and was used in influential early work on convolutional networks and recurrent networks.

However, tanh still saturates at large magnitudes, so the vanishing gradient problem persists. Training very deep networks remained difficult.

## ReLU (2010)

For decades, sigmoid and tanh dominated. Then came a surprisingly simple alternative that changed everything.

$$\text{ReLU}(z) = \max(0, z)$$

<ActivationGraph type="relu" yRange={[-1, 5]} />

ReLU (the Rectified Linear Unit) outputs zero for negative inputs and passes positive inputs unchanged <Cite authors="Nair & Hinton" year="2010" url="https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf" />. It never saturates for positive values, so gradients flow freely. It's also computationally cheap, requiring no exponentials or divisions, just a threshold check.

ReLU made training deep networks dramatically easier. It became the default activation for most architectures.

The main drawback is "dying ReLU": if a neuron's inputs consistently land in the negative region, it outputs zero and receives zero gradients. Once dead, it stays dead.

## Leaky ReLU

A simple fix for dying neurons is to allow a small gradient for negative inputs.

$$\text{LeakyReLU}(z) = \max(\alpha z, z), \quad \alpha \approx 0.01$$

<ActivationGraph type="leakyRelu" yRange={[-1, 5]} />

Instead of flatlining at zero, negative inputs produce a small negative output. This keeps neurons alive during training while preserving ReLU's benefits.

## GELU (2016)

Modern architectures like Transformers use smoother alternatives <Cite authors="Hendrycks & Gimpel" year="2016" url="https://arxiv.org/abs/1606.08415" />. GELU multiplies the input by a value between 0 and 1 that depends on how large the input is.

$$\text{GELU}(z) = z \cdot \Phi(z)$$

Here $\Phi(z)$ is the probability that a random sample from a standard normal distribution is less than $z$.

$$\Phi(z) = \frac{1}{2}\left[1 + \text{erf}\left(\frac{z}{\sqrt{2}}\right)\right]$$

where $\text{erf}$ is the error function. For large positive $z$, this probability approaches 1, so the input passes through almost unchanged. For large negative $z$, the probability approaches 0, so the input is nearly zeroed. For values near zero, the input is partially scaled down.

In practice, a fast approximation is often used:

$$\text{GELU}(z) \approx 0.5z\left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}\left(z + 0.044715z^3\right)\right]\right)$$

<ActivationGraph type="gelu" yRange={[-1, 5]} />

Unlike ReLU's hard cutoff at zero, GELU smoothly transitions. Small negative values are partially passed through rather than completely zeroed. This matches the behavior of stochastic regularization techniques like dropout and has become standard in large language models.

## Summary

Activation functions exist to introduce non-linearity. The story is one of gradual refinement, from biologically-inspired step functions that couldn't be trained with gradients, to smooth sigmoids that enabled backpropagation, to ReLU that finally made deep networks practical, to modern variants tuned for specific architectures.

The biological motivation faded long ago. What remains is a simple requirement: break linearity so that depth means something.
