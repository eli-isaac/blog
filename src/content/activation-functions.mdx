export const meta = {
  title: "Activation Functions",
  subtitle: "A chronological tour through non-linearity in neural networks",
  date: "2025-12-25",
  authors: ["Eli Plutchok", "Isaac Trenk"],
  slug: "activation-functions-in-neural-networks"
}

## Why activation functions?

An activation function is a **non-linear** function applied inside a neural network, typically right after a linear transformation. A single layer computes:

$$z = Wx + b, \quad a = \phi(z)$$

where $W$ and $b$ define a linear map and $\phi$ is the activation function.

Why is non-linearity necessary? Without it, stacking layers achieves nothing. Two linear layers:

$$h = W_1x + b_1, \quad y = W_2h + b_2$$

collapse into a single linear map:

$$y = (W_2W_1)x + (W_2b_1 + b_2)$$

No matter how deep the network, the result is equivalent to one layer. Linear functions cannot model thresholds, varying rates of change, or interactions between inputs. They are restricted to simple additive effects like $ax + by$.

Non-linear activations break this collapse. Once applied, layers can no longer be merged. This allows deep networks to build rich representations and approximate complex functions <Cite authors="Cybenko" year="1989" url="https://doi.org/10.1007/BF02551274" />.

<NeuralNetworkDemo />

The earliest activation functions were inspired by biology. Over time, that motivation faded, and what mattered was the math. Here's how they evolved.

## Step function (1943)

The first neural network models aimed to mimic biological neurons. A neuron receives inputs, weighs them, and "fires" if the total exceeds a threshold. McCulloch and Pitts formalized this as <Cite authors="McCulloch & Pitts" year="1943" url="https://doi.org/10.1007/BF02478259" />:

$$y = \text{step}\left(\sum_i w_i x_i - \theta\right)$$

where $w_i$ are weights, $x_i$ are inputs, and $\theta$ is a threshold. The step function outputs 1 if the weighted sum exceeds the threshold, and 0 otherwise:

$$\text{step}(z) = \begin{cases} 1 & z \ge 0 \\ 0 & z < 0 \end{cases}$$

<ActivationGraph type="step" />

Rosenblatt's perceptron <Cite authors="Rosenblatt" year="1958" url="https://doi.org/10.1037/h0042519" /> used this activation and could learn simple classifications through a weight update rule.

But the step function has a fatal flaw for gradient-based learning. Its derivative is zero everywhere except at the threshold, where it's undefined. This creates two problems: gradients provide no signal for how to adjust weights, and even if we somehow knew which direction to move, the output jumps discontinuously. There's no way to make small, controlled updates. Backpropagation cannot work under these conditions.

## Sigmoid (1986)

The solution was to smooth the step into a differentiable curve. The **logistic sigmoid** became the standard activation for early multi-layer networks trained with backpropagation <Cite authors="Rumelhart, Hinton & Williams" year="1986" url="https://doi.org/10.1038/323533a0" />:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

<ActivationGraph type="sigmoid" />

Sigmoid behaves like a soft step: large negative inputs map near 0, large positive inputs map near 1, with a smooth transition between. Its derivative is well-defined everywhere:

$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$

This allowed gradients to flow backward through networks, enabling training of deeper architectures. The output range $(0, 1)$ also made sigmoid natural for probabilistic outputs.

However, sigmoid saturates at extreme values, so the derivative approaches zero, causing gradients to vanish during backpropagation. This made training deep networks slow and difficult.

## Tanh

A simple improvement: shift sigmoid to be zero-centered.

$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

<ActivationGraph type="tanh" yRange={[-1.5, 1.5]} />

Tanh maps inputs to $(-1, 1)$ instead of $(0, 1)$. Zero-centered outputs often help optimization since gradients don't systematically push weights in one direction. But tanh still saturates at large magnitudes, so the vanishing gradient problem persists.

## ReLU (2010)

For decades, sigmoid and tanh dominated. Then came a surprisingly simple alternative that changed everything:

$$\text{ReLU}(z) = \max(0, z)$$

<ActivationGraph type="relu" yRange={[-1, 5]} />

ReLU (the Rectified Linear Unit) outputs zero for negative inputs and passes positive inputs unchanged <Cite authors="Nair & Hinton" year="2010" url="https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf" />. It never saturates for positive values, so gradients flow freely. It's also computationally trivial, just a comparison and a maximum.

ReLU made training deep networks dramatically easier. It became the default activation for most architectures.

The main drawback is "dying ReLU": if a neuron's inputs consistently land in the negative region, it outputs zero and receives zero gradients. Once dead, it stays dead.

## Leaky ReLU

A simple fix for dying neurons: allow a small gradient for negative inputs.

$$\text{LeakyReLU}(z) = \max(\alpha z, z), \quad \alpha \approx 0.01$$

<ActivationGraph type="leakyRelu" yRange={[-1, 5]} />

Instead of flatlining at zero, negative inputs produce a small negative output. This keeps neurons alive during training while preserving ReLU's benefits.

## GELU (2016)

Modern architectures like Transformers use smoother alternatives. GELU gates inputs probabilistically based on their magnitude <Cite authors="Hendrycks & Gimpel" year="2016" url="https://arxiv.org/abs/1606.08415" />:

$$\text{GELU}(z) = z \cdot \Phi(z)$$

where $\Phi(z)$ is the cumulative distribution function of the standard normal.

<ActivationGraph type="gelu" yRange={[-1, 5]} />

Unlike ReLU's hard cutoff at zero, GELU smoothly transitions. Small negative values are partially passed through rather than completely zeroed. This matches the behavior of stochastic regularization techniques like dropout and has become standard in large language models.

## Summary

Activation functions exist to introduce non-linearity. The story is one of gradual refinement: from biologically-inspired step functions that couldn't be trained with gradients, to smooth sigmoids that enabled backpropagation, to ReLU that finally made deep networks practical, to modern variants tuned for specific architectures.

The biological motivation faded long ago. What remains is a simple requirement: break linearity so that depth means something.
